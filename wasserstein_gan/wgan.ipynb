{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchinfo\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, generator: nn.Module, critic: nn.Module, lr: float, gp_weight: float, critic_iters: int, img_save_path: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gp_weight = gp_weight\n",
    "        self.critic_iters = critic_iters\n",
    "\n",
    "        # for manual backward\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.img_save_path = img_save_path\n",
    "\n",
    "        # sample z for logging\n",
    "        logging_z = torch.randn((4, 100, 1, 1), device=self.device)\n",
    "        self.register_buffer(\"logging_z\", logging_z)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def _gradient_penalty(self, x_real: torch.Tensor, x_fake: torch.Tensor):\n",
    "        batch_size = x_real.shape[0]\n",
    "\n",
    "        # generate epsilon for each batch\n",
    "        epsilon = torch.rand((batch_size, 1, 1, 1), device=self.device)\n",
    "\n",
    "        # interpolated image, epsilon of size [b, 1, 1, 1], x [b, c, h, w], epsilon is broadcasting\n",
    "        x_interpolated = epsilon * x_real + (1 - epsilon) * x_fake\n",
    "\n",
    "        crt_interpolated = self.critic(x_interpolated)\n",
    "\n",
    "        # calculate gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=crt_interpolated,\n",
    "            inputs=x_interpolated,\n",
    "            grad_outputs=torch.ones_like(crt_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "\n",
    "        # calculating gradient norm\n",
    "        grad_norm = torch.norm(gradients, p=2, dim=1)\n",
    "        # gradient penalty\n",
    "        gp = torch.mean((grad_norm - 1) ** 2)\n",
    "\n",
    "        return gp\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get optimizers\n",
    "        opt_generator, opt_critic = self.optimizers()\n",
    "\n",
    "        # get training data\n",
    "        x_real, _ = batch\n",
    "\n",
    "        ##################################\n",
    "        # CRITIC\n",
    "        ##################################\n",
    "\n",
    "        # critic update iterations per one generator update\n",
    "        for _ in range(self.critic_iters):\n",
    "            # sample z\n",
    "            z = torch.randn((x_real.shape[0], 100, 1, 1), device=self.device)\n",
    "\n",
    "            # generate fake img\n",
    "            x_fake = self.generator(z)\n",
    "\n",
    "            crt_real = self.critic(x_real)\n",
    "            crt_fake = self.critic(x_fake)\n",
    "\n",
    "            crt_wasserstein_distance = -torch.mean(crt_real - crt_fake)\n",
    "            gp = self._gradient_penalty(x_real, x_fake)\n",
    "\n",
    "            crt_loss = crt_wasserstein_distance + self.gp_weight * gp\n",
    "\n",
    "            opt_critic.zero_grad()\n",
    "            self.manual_backward(crt_loss)\n",
    "            opt_critic.step()\n",
    "\n",
    "        ##################################\n",
    "        # GENERATOR\n",
    "        ##################################\n",
    "\n",
    "        # sample z\n",
    "        z = torch.randn((x_real.shape[0], 100, 1, 1), device=self.device)\n",
    "\n",
    "        # generate fake img\n",
    "        x_fake = self.generator(z)\n",
    "        crt_fake = self.critic(x_fake)\n",
    "\n",
    "        gen_loss = -torch.mean(crt_fake)\n",
    "\n",
    "        opt_generator.zero_grad()\n",
    "        self.manual_backward(gen_loss)\n",
    "        opt_generator.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train_epoch_end(self):\n",
    "        # generate fake img\n",
    "        generated = self.generator(self.logging_z)\n",
    "        grid = torchvision.utils.make_grid(generated)\n",
    "\n",
    "        # save generated image grid\n",
    "        torchvision.utils.save_image(grid, f\"{self.img_save_path}/img_epoch_{self.current_epoch}.jpg\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_generator = torch.optim.Adam(self.generator.parameters(), self.lr)\n",
    "        opt_critic = torch.optim.Adam(self.critic.parameters(), self.lr)\n",
    "\n",
    "        return opt_generator, opt_critic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, upscale: bool = False, **kwargs):\n",
    "        \"\"\"\n",
    "        Conv Block for Generator and Critic architectures\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): input channels\n",
    "            out_channels (int): output channels\n",
    "            upscale (bool, optional): if True upscales image by a factor of 2. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.upscale = upscale\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # upscaling img if true\n",
    "        if self.upscale:\n",
    "            x = F.interpolate(x, scale_factor=2.0)\n",
    "        \n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels: int, intermediate_channels: List[int], out_channels: int):\n",
    "        \"\"\"\n",
    "        Generator model\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): input channels\n",
    "            intermediate_channels (List[int]): list containing number of intermediate channels in each layer of the generator\n",
    "            out_channels (int): output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.generator = self._build_architecture()\n",
    "\n",
    "    def _build_architecture(self):\n",
    "        layers = []\n",
    "\n",
    "        layers += [ConvBlock(self.in_channels, self.intermediate_channels[0], upscale=True, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        for ins, outs in zip(self.intermediate_channels[:-1], self.intermediate_channels[1:]):\n",
    "            layers += [ConvBlock(ins, outs, upscale=True, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        layers += [ConvBlock(self.intermediate_channels[-1], self.out_channels, upscale=False, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        return self.generator(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_channels: int, intermediate_channels: List[int]):\n",
    "        \"\"\"\n",
    "        Critic architecture\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): input channels\n",
    "            intermediate_channels (List[int]): list containing number of intermediate channels in each layer of the generator\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "\n",
    "        self.critic = self._build_architecture()\n",
    "\n",
    "    def _build_architecture(self):\n",
    "        layers = []\n",
    "\n",
    "        layers += [ConvBlock(self.in_channels, self.intermediate_channels[0], upscale = False, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        for ins, outs in zip(self.intermediate_channels[:-1], self.intermediate_channels[1:]):\n",
    "            layers += [ConvBlock(ins, outs, upscale = False, kernel_size=4, stride=2, padding=1)]\n",
    "\n",
    "        layers += [ConvBlock(self.intermediate_channels[-1], out_channels=1, upscale = False, kernel_size=3, stride=1, padding=1)]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.critic(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgTransform:\n",
    "    def __init__(self):\n",
    "        self.transforms = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.transforms(image=np.array(img))[\"image\"]\n",
    "\n",
    "transform = ImgTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CelebA(download=True, root=\"../../datasets/celeb_a\", transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_gen = [128, 128, 256, 256, 256, 512, 512, 512]\n",
    "generator = Generator(100, layers_gen, 3)\n",
    "\n",
    "layers_crt = [64, 128, 128, 256, 256]\n",
    "critic = Critic(3, layers_crt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | generator | Generator | 7.7 M \n",
      "1 | critic    | Critic    | 2.0 M \n",
      "----------------------------------------\n",
      "9.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 M     Total params\n",
      "38.518    Total estimated model params size (MB)\n",
      "/home/jasiek/anaconda3/envs/machine-learning-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70066b9f1c514a3fa026dbdf8f76f6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# logger = WandbLogger(\"wgan\")\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=100)\n",
    "learner = Learner(generator, critic, 3e-4, critic_iters=5, gp_weight=10, img_save_path=\"sampled_imgs\")\n",
    "trainer.fit(learner, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee2ab588395e765446baa3aa07d10ec7d1eb1a0000e21d069248c85ac3e17b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
