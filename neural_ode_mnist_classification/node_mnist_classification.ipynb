{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.nn import Augmenter\n",
    "from torchdyn.utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, lr: float, l2: float, t_span: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.t_span = t_span\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(multiclass=True, num_classes=10)\n",
    "        self.f1_score = torchmetrics.F1Score(multiclass=True, num_classes=10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x, self.t_span)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        f1 = self.f1_score(y_hat, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train/loss\": loss,\n",
    "                \"train/acc\": acc,\n",
    "                \"train/f1\": f1\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        f1 = self.f1_score(y_hat, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/loss\": loss,\n",
    "                \"val/acc\": acc,\n",
    "                \"val/f1\": f1\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        f1 = self.f1_score(y_hat, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test/loss\": loss,\n",
    "                \"test/acc\": acc,\n",
    "                \"test/f1\": f1\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "        \n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODEClassifier(nn.Module):\n",
    "    def __init__(self, preprocessing_layer: nn.Module, vector_field: nn.Module, classifier: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.preprocessing_layer = preprocessing_layer\n",
    "        self.node = NeuralODE(vector_field, solver=\"dopri5\", order=1, sensitivity=\"adjoint\")\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_span: torch.Tensor):\n",
    "        x = self.preprocessing_layer(x)\n",
    "\n",
    "        _, x = self.node(x, t_span)\n",
    "\n",
    "        return self.classifier(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, bias=False, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "preprocessing_layer = Augmenter(augment_idx=1, augment_dims=15)\n",
    "\n",
    "f = nn.Sequential(\n",
    "    ConvBlock(16, 16, kernel_size=3, padding=1),\n",
    "    ConvBlock(16, 16, kernel_size=3, padding=1),\n",
    "    ConvBlock(16, 16, kernel_size=3, padding=1)\n",
    ")\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    ConvBlock(16, 1, kernel_size=3, padding=1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 10)\n",
    ")\n",
    "\n",
    "model = NODEClassifier(preprocessing_layer, f, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms:\n",
    "    def __init__(self):\n",
    "        self.transforms = A.Compose([\n",
    "            A.Normalize(mean=0.5, std=0.5),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.transforms(image=np.array(img))[\"image\"]\n",
    "\n",
    "transform = Transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root=\"dataset/\", train=True, download=True, transform=transform)\n",
    "mnist_val = MNIST(root=\"dataset/\", train=False, download=True, transform=transform)\n",
    "\n",
    "loader_train = DataLoader(mnist_train, batch_size=32, shuffle=True, num_workers=8)\n",
    "loader_val = DataLoader(mnist_val, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/jasiek/python-projects/deep-learning-in-pytorch/node_mnist_classification/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params\n",
      "--------------------------------------------\n",
      "0 | model    | NODEClassifier | 15.0 K\n",
      "1 | accuracy | Accuracy       | 0     \n",
      "2 | f1_score | F1Score        | 0     \n",
      "--------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb2684e2ba449feab0b320d269da35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasiek/anaconda3/envs/data-science-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9056e0dd1d4a9c9dfdeb6801c7e90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasiek/anaconda3/envs/data-science-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(model, lr=3e-4, l2=0.001)\n",
    "trainer = pl.Trainer(gpus=-1)\n",
    "trainer.fit(learner, train_dataloaders=loader_train, val_dataloaders=loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data-science-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "293d8ec2af927318b4bcafbc1915b70699548b10be33d20cca26149099642076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
