{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pretty_midi\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building WaveNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, dilation: int = 1, **kwargs):\n",
    "        \"\"\"\n",
    "        Implementation of Causal Convolution 1d, computes 1d Convolution with mask so that values are only influenced by preceeding values\n",
    "\n",
    "        :param int in_channels: input channels\n",
    "        :param int out_channels: output channels\n",
    "        :param int kernel_size: size of filter kernel\n",
    "        :param int dilation: dilation of kernel, defaults to 1 <- no dilation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # calculating same padding based on kernel_size and dilation\n",
    "        padding = dilation * (kernel_size-1) // 2\n",
    "\n",
    "        # creating mask\n",
    "        mask = torch.ones(kernel_size)\n",
    "        mask[kernel_size//2+1:] = 0\n",
    "        self.register_buffer(\"mask\", mask[None])\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # applying mask to filter weights\n",
    "        self.conv1d.weight.data *= self.mask\n",
    "\n",
    "        return self.conv1d(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated Residual Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, kernel_size: int, dilation: int):\n",
    "        \"\"\"\n",
    "        Implementation of Gated Residual Conv Block with Causal Convolution Layers \n",
    "\n",
    "        :param int in_channels: input channels\n",
    "        :param int hidden_channels: intermediate channels between CasusalConv layer and Conv1x1\n",
    "        :param int out_channels: output channels\n",
    "        :param int kernel_size: size of filter kernel\n",
    "        :param int dilation: dilation of kernel\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dilated_conv = CausalConv(in_channels, 2*hidden_channels, kernel_size, dilation=dilation, bias=False)\n",
    "        self.gn = nn.GroupNorm(num_groups=32, num_channels=2*hidden_channels)\n",
    "        self.conv_1x1 = nn.Conv1d(hidden_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # applying causal conv\n",
    "        dilated = self.dilated_conv(x)\n",
    "\n",
    "        dilated = self.gn(dilated)\n",
    "\n",
    "        # spliting channel dim into value and gate\n",
    "        value, gate = dilated.chunk(2, dim=1)\n",
    "\n",
    "        # gate\n",
    "        gated_value = torch.tanh(value) * torch.sigmoid(gate)\n",
    "\n",
    "        # output conv\n",
    "        output = self.conv_1x1(gated_value)\n",
    "        # residual connection\n",
    "        residual_output = output + x\n",
    "\n",
    "        # output of residual connection and output of skip connection\n",
    "        return residual_output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated Conv Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvStack(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, kernel_size: int, num_residual_blocks: int):\n",
    "        \"\"\"\n",
    "        Stack of Gated Residual Conv Blocks with dilation doubling at each step\n",
    "\n",
    "        :param int in_channels: input channels\n",
    "        :param int hidden_channels: intermediate channels\n",
    "        :param int out_channels: output channels\n",
    "        :param int kernel_size: size of filter kernels\n",
    "        :param int num_residual_blocks: num of conv blocks in stack\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # generating dilations -> 1, 2, 4, 8, 16, ...\n",
    "        dilations = [2**i for i in range(num_residual_blocks)]\n",
    "\n",
    "        self.conv_stack = nn.ModuleList(\n",
    "            [GatedResidualConvBlock(in_channels, hidden_channels, out_channels, kernel_size, dilations[i]) for i in range(num_residual_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        skip_connections = []\n",
    "\n",
    "        for layer in self.conv_stack:\n",
    "            x, skip_connection = layer(x)\n",
    "\n",
    "            skip_connections.append(skip_connection)\n",
    "\n",
    "        # residual connection to next conv block, sum of skip connections from stack\n",
    "        return x, torch.stack(skip_connections, dim=-1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, kernel_size: int, num_stacks: int, num_residual_blocks_in_stack: int):\n",
    "        \"\"\"\n",
    "        Implementation of WaveNet Architecture\n",
    "\n",
    "        :param int in_channels: input channels\n",
    "        :param int hidden_channels: intermediate channels\n",
    "        :param int out_channels: output channels\n",
    "        :param int kernel_size: size of filter kernels\n",
    "        :param int num_stacks: num of stacks\n",
    "        :param int num_residual_blocks_in_stack: num of gated residual conv blocks in each stack\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.causal_conv = CausalConv(in_channels, hidden_channels, kernel_size)\n",
    "        self.gated_conv_stacks = nn.ModuleList(\n",
    "            [GatedConvStack(hidden_channels, hidden_channels, hidden_channels, kernel_size, num_residual_blocks_in_stack) for _ in range(num_stacks)]\n",
    "        )\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, 1, 1, 0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_channels, out_channels, 1, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        skip_connections = []\n",
    "        \n",
    "        x = self.causal_conv(x)\n",
    "\n",
    "        for stack in self.gated_conv_stacks:\n",
    "            x, skip_connection = stack(x)\n",
    "\n",
    "            skip_connections.append(skip_connection)\n",
    "\n",
    "        # sum of all skip connection outputs\n",
    "        output = torch.stack(skip_connections, dim=-1).sum(dim=-1)\n",
    "\n",
    "        return self.output_block(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MelodyWaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Output Block for Melody WaveNet\n",
    "\n",
    "        :param int channels: input channels\n",
    "        :param int num_classes: number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            CausalConv(channels, channels, kernel_size=5, dilation=1, bias=False),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.GELU(),\n",
    "            CausalConv(channels, channels, kernel_size=5, dilation=2, bias=False),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(channels, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.output(x)\n",
    "\n",
    "class MelodyWaveNet(nn.Module):\n",
    "    def __init__(self, embedding_dims: int, channels: int, kernel_size: int, num_stacks: int, num_residual_blocks_in_stack: int):\n",
    "        \"\"\"\n",
    "        Implementation of WaveNet for generating midi sequences\n",
    "\n",
    "        :param int embedding_dims: embedding dimension for each feature\n",
    "        :param int channels: intermediate channels\n",
    "        :param int out_channels: output channels\n",
    "        :param int kernel_size: size of filter kernels\n",
    "        :param int num_stacks: num of stacks\n",
    "        :param int num_residual_blocks_in_stack: num of gated residual conv blocks in each stack\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # embeddings for features, pitch and velocity have values between 0-128\n",
    "        # duration and step between 0-7 corresponding to different note lengths -> 0 - 0 length, 1 - 32th note, 2 - 16th note, 3 - 8th note, etc.\n",
    "        # for simplicity dotted notes and triplets are omitted\n",
    "        self.embedding_pitch = nn.Embedding(num_embeddings=129, embedding_dim=embedding_dims, padding_idx=0)\n",
    "        self.embedding_velocity = nn.Embedding(num_embeddings=129, embedding_dim=embedding_dims, padding_idx=0)\n",
    "        self.embedding_duration = nn.Embedding(num_embeddings=8, embedding_dim=embedding_dims, padding_idx=0)\n",
    "        self.embedding_step = nn.Embedding(num_embeddings=8, embedding_dim=embedding_dims, padding_idx=0)\n",
    "\n",
    "        self.wavenet = WaveNet(4*embedding_dims, channels, channels, kernel_size, num_stacks, num_residual_blocks_in_stack)\n",
    "\n",
    "        # output shape [batch_size, num_classes, seq_len]\n",
    "        self.output_pitch = OutputBlock(channels, num_classes=129)\n",
    "        self.output_velocity = OutputBlock(channels, num_classes=129)\n",
    "        self.output_duration = OutputBlock(channels, num_classes=8)\n",
    "        self.output_step = OutputBlock(channels, num_classes=8)\n",
    "\n",
    "    def forward(self, pitch: torch.Tensor, velocity: torch.Tensor, duration: torch.Tensor, step: torch.Tensor):\n",
    "        # pitch, velocity, duration and step have shapes of [batch_size, seq_len]\n",
    "\n",
    "        # embedding each feature, shapes after embedding -> [batch_size, seq_len, embedding_dim]\n",
    "        pitch_embed = self.embedding_pitch(pitch)\n",
    "        velocity_embed = self.embedding_velocity(velocity)\n",
    "        duration_embed = self.embedding_duration(duration)\n",
    "        step_embed = self.embedding_step(step)\n",
    "\n",
    "        # concatenating features on embedding dim -> [batch_size, seq_len, 4*embedding_dim]\n",
    "        # permuting so embedding is counted as channels -> [batch_size, 4*embedding_dim, seq_len]\n",
    "        features = torch.cat([pitch_embed, velocity_embed, duration_embed, step_embed], dim=-1).permute(0, 2, 1)\n",
    "\n",
    "        # passing through WaveNet\n",
    "        x = self.wavenet(features)\n",
    "\n",
    "        # shapes after output layers -> [batch_size, num_classes, seq_len]\n",
    "        pitch = self.output_pitch(x)\n",
    "        velocity = self.output_velocity(x)\n",
    "        duration = self.output_duration(x)\n",
    "        step = self.output_step(x)\n",
    "\n",
    "        return pitch, velocity, duration, step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Wrapper for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyWaveNetLit(pl.LightningModule):\n",
    "    def __init__(\n",
    "                self, embedding_dims: int, channels: int, kernel_size: int, \n",
    "                num_stacks: int, num_residual_blocks_in_stack: int, lr: float, loss_lambdas: List[int], l2: float\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # model\n",
    "        self.melody_wavenet = MelodyWaveNet(embedding_dims, channels, kernel_size, num_stacks, num_residual_blocks_in_stack)\n",
    "\n",
    "    def forward(self, pitch: torch.Tensor, velocity: torch.Tensor, duration: torch.Tensor, step: torch.Tensor):\n",
    "        return self.melody_wavenet(pitch, velocity, duration, step)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.melody_wavenet.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), weight_decay = self.hparams.l2)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.1)\n",
    "\n",
    "        return optimizer, lr_scheduler\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # split features\n",
    "        # shapes [N, seq_len]\n",
    "        pitch, velocity, duration, step = batch\n",
    "\n",
    "        # shapes [N, num_classes, seq_len]\n",
    "        pitch_pred, velocity_pred, duration_pred, step_pred = self.forward(pitch[:,:,:-1], velocity[:,:,:-1], duration[:,:,:-1], step[:,:,:-1])\n",
    "\n",
    "        nll_pitch = F.cross_entropy(pitch_pred, pitch[:,:,1:])\n",
    "        nll_velocity = F.cross_entropy(velocity_pred, velocity[:,:,1:])\n",
    "        nll_duration = F.cross_entropy(duration_pred, duration[:,:,1:])\n",
    "        nll_step = F.cross_entropy(step_pred, step[:,:,1:])\n",
    "\n",
    "        loss = (\n",
    "             self.hparams.loss_lambdas[0] * nll_pitch + \n",
    "             self.hparams.loss_lambdas[1] * nll_velocity + \n",
    "             self.hparams.loss_lambdas[2] * nll_duration + \n",
    "             self.hparams.loss_lambdas[3] * nll_step\n",
    "        )\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/loss_pitch\": nll_pitch,\n",
    "                \"val/loss_velocity\": nll_velocity,\n",
    "                \"val/loss_duration\": nll_duration,\n",
    "                \"val/loss_step\": nll_step,\n",
    "                \"val/total_loss\": loss\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pitch, velocity, duration, step = batch\n",
    "\n",
    "        # shapes [N, num_classes, seq_len]\n",
    "        pitch_pred, velocity_pred, duration_pred, step_pred = self.forward(pitch[:,:,:-1], velocity[:,:,:-1], duration[:,:,:-1], step[:,:,:-1])\n",
    "\n",
    "        nll_pitch = F.cross_entropy(pitch_pred, pitch[:,:,1:])\n",
    "        nll_velocity = F.cross_entropy(velocity_pred, velocity[:,:,1:])\n",
    "        nll_duration = F.cross_entropy(duration_pred, duration[:,:,1:])\n",
    "        nll_step = F.cross_entropy(step_pred, step[:,:,1:])\n",
    "\n",
    "        loss = (\n",
    "             self.hparams.loss_lambdas[0] * nll_pitch + \n",
    "             self.hparams.loss_lambdas[1] * nll_velocity + \n",
    "             self.hparams.loss_lambdas[2] * nll_duration + \n",
    "             self.hparams.loss_lambdas[3] * nll_step\n",
    "        )\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/loss_pitch\": nll_pitch,\n",
    "                \"val/loss_velocity\": nll_velocity,\n",
    "                \"val/loss_duration\": nll_duration,\n",
    "                \"val/loss_step\": nll_step,\n",
    "                \"val/total_loss\": loss\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, midi_file_list: list, seq_len: int):\n",
    "        self.midi_file_list = midi_file_list\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # loading data\n",
    "        data = pd.read_csv(self.midi_file_list[index]).to_numpy(dtype=np.int64)\n",
    "\n",
    "        # grab random starting index\n",
    "        start_idx = random.randint(0, len(data)-self.seq_len-1)\n",
    "        # get slice of data\n",
    "        data = data[start_idx:start_idx+self.seq_len]\n",
    "\n",
    "        # padding values\n",
    "        data_len = len(data)\n",
    "        if data_len < self.seq_len:\n",
    "            data = np.pad(data, ((0, self.seq_len-data_len), (0, 0)))\n",
    "\n",
    "        data_torch = torch.from_numpy(data)\n",
    "\n",
    "        # returning pitch, velocity, duration and step\n",
    "        return data_torch[:, 0], data_torch[:, 1], data_torch[:, 2], data_torch[:, 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    lr=3e-4,\n",
    "    l2=0.01,\n",
    "    loss_lambdas=[2.0, 1.0, 1.0, 1.0],\n",
    "    seq_len = 512\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = MelodyWaveNetLit(\n",
    "    embedding_dims=32, channels=128, kernel_size=5, num_stacks=3, num_residual_blocks_in_stack=4, \n",
    "    lr=config[\"lr\"], loss_lambdas=config[\"loss_lambdas\"], l2=config[\"l2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            # nn.init.xavier_uniform_(m.bias.data)\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(model, config, train_filepaths, val_filepaths):\n",
    "    # logger\n",
    "    logger = WandbLogger(name=\"melody_wavenet\", project=\"melody_wavenet\")\n",
    "\n",
    "    # setting up trainer\n",
    "    trainer = pl.Trainer(logger=logger, log_every_n_steps=50, accelerator='gpu', devices=-1, max_epochs=config[\"epochs\"], precision=16)\n",
    "\n",
    "    # datasets\n",
    "    train_set = MidiDataset(train_filepaths, seq_len=config[\"seq_len\"])\n",
    "    val_set = MidiDataset(val_filepaths, seq_len=config[\"seq_len\"])\n",
    "\n",
    "    # dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # training\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    torch.save(model.melody_wavenet, \"melody_generator.pt\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffling data\n",
    "data_filepaths = glob.glob(\"../../wavenet/extracted_data/*\")\n",
    "random.shuffle(data_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are about 1200 files, so 1000 go to train and the rest goes to validation\n",
    "train_file_list = data_filepaths[:1000]\n",
    "val_file_list = data_filepaths[1000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline(model, config, train_file_list, val_file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melody Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyGenerator:\n",
    "    def __init__(self, model: nn.Module, temperature: list):\n",
    "        self.model = model.eval()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_sequence(self, sequence_len: int, seq: np.ndarray = None):\n",
    "        # sequence shape: [1, seq_len]\n",
    "        if seq is None:\n",
    "            pitch, velocity, duration, step = (torch.zeros((1, sequence_len), dtype=torch.long) for _ in range(4))\n",
    "        else:\n",
    "            # generate tensors with conditioned values and padded to seq_len\n",
    "            pitch, velocity, duration, step = (torch.LongTensor(np.pad(seq[i], (0, sequence_len-len(seq[i])))).unsqueeze(0) for i in range(4))\n",
    "\n",
    "        start_idx = 0 if seq is None else len(seq[0])\n",
    "\n",
    "        for i in tqdm(range(start_idx, sequence_len)):\n",
    "            \n",
    "            # passing only previous values to speed up computation, out shape [batch_size, classes, seq_len]\n",
    "            pitch_pred, velocity_pred, duration_pred, step_pred = self.model(pitch[:,:i], velocity[:,:i], duration[:,:i], step[:,:i])\n",
    "\n",
    "            pitch[:,i] = self._sample_with_temperature(pitch_pred[:,:,-1], self.temperature[0])\n",
    "            velocity[:,i] = self._sample_with_temperature(velocity_pred[:,:,-1], self.temperature[1])\n",
    "            duration[:,i] = self._sample_with_temperature(duration_pred[:,:,-1], self.temperature[2])\n",
    "            step[:,i] = self._sample_with_temperature(step_pred[:,:,-1], self.temperature[3])\n",
    "\n",
    "        return torch.cat([pitch, velocity, duration, step], dim=0)\n",
    "        \n",
    "    def _sample_with_temperature(self, values: torch.Tensor, temperature: float):\n",
    "        predictions = values / temperature\n",
    "        predictions[:, 0] = -9999\n",
    "\n",
    "        probabilities = torch.softmax(predictions, dim=1)\n",
    "\n",
    "        return torch.multinomial(probabilities, num_samples=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding generated data to MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notes2Midi:\n",
    "    def __init__(self):\n",
    "        # mapping to convert idx to note length\n",
    "        self.mapping = {\n",
    "            1: 0.0,\n",
    "            2: 0.125,\n",
    "            3: 0.25,\n",
    "            4: 0.5,\n",
    "            5: 1.0,\n",
    "            6: 2.0,\n",
    "            7: 4.0\n",
    "        }\n",
    "\n",
    "    def save_sequence_as_midi(self, sequence: np.ndarray, tempo: float, save_path: str):\n",
    "        pm = pretty_midi.PrettyMIDI()\n",
    "        instrument = pretty_midi.Instrument(program=0)\n",
    "\n",
    "        # calculate quarter note length based on tempo\n",
    "        quarter_note_len = 60.0 / tempo\n",
    "\n",
    "        previous_start = 0.0\n",
    "\n",
    "        pitch, velocity, duration, step = sequence[0], sequence[1], sequence[2], sequence[3]\n",
    "\n",
    "        for p, v, d, s in zip(pitch, velocity, duration, step):\n",
    "\n",
    "            # calulate note start and note end\n",
    "            start = previous_start + self.mapping[s] * quarter_note_len\n",
    "            end = start + self.mapping[d] * quarter_note_len\n",
    "\n",
    "            note = pretty_midi.Note(velocity=v-1, pitch=p-1, start=start, end=end)\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "            previous_start = start\n",
    "\n",
    "        pm.instruments.append(instrument)\n",
    "        pm.write(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"melody_generator_3.pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MelodyGenerator(model, temperature=[1,1,1,1])\n",
    "n2m = Notes2Midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750ca1e3d2c142a8b2e079b075af6487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_seq = mg.generate_sequence(512).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2m.save_sequence_as_midi(gen_seq, 120, \"generated/14.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee2ab588395e765446baa3aa07d10ec7d1eb1a0000e21d069248c85ac3e17b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
